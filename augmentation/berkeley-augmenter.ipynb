{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabe1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import itertools\n",
    "from typing import List \n",
    "import instructor\n",
    "from vertexai.generative_models import GenerativeModel \n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = \"terraform-training-387507\"\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = \"global\"\n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = \"true\"\n",
    "\n",
    "output_dir = \"berkeley\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd22717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw/berkeley_dataset_multi.csv\")\n",
    "data  = data[data['contains_hate'] == 1]\n",
    "selected = ['age', 'disability']\n",
    "filter = \" or \".join(f\"{col} == 1\" for col in selected)\n",
    "data = data.query(filter)\n",
    "data = data.reset_index(drop=True)\n",
    "ignored_cols = ['text', 'hatespeech', 'contains_hate']\n",
    "data['labels'] = data.apply(lambda x: ','.join(k for k in x.keys() if k not in ignored_cols and x[k] == 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a56d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert in hate speech detection and generation\n",
    "Below there are instances of hate speech comments. Each of them can have one or more labels assigned to it, seperated by comma. This corresponds to multilabel classification task.\n",
    "\n",
    "```\n",
    "{inputs}\n",
    "```\n",
    "\n",
    "For each comment, generate at least 5 comments which have the same semantic meaning and the same labels assigned to them. Use rude, foul and explicit language where applicable.\n",
    "Very long comments should be summarized or paraphrased into shorter ones. The generated comments should vary in tone, severity, or lexical variation. Return them in JSON format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03b23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateExamples(BaseModel):\n",
    "    comment: str\n",
    "    index: int\n",
    "    label: List[str]\n",
    "    generated_comments: List[str]\n",
    "\n",
    "\n",
    "client = instructor.from_vertexai(\n",
    "    client=GenerativeModel(\"gemini-2.5-flash\"),\n",
    "    mode=instructor.Mode.VERTEXAI_TOOLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e92d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 970 - 1040: : 970it [00:01, 41.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while processing range 900 - 970: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1040 - 1110: : 1040it [06:19,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while processing range 970 - 1040: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1180 - 1250: : 1180it [08:11,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while processing range 1110 - 1180: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1390 - 1450: : 1460it [25:37,  2.74s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 70\n",
    "start = 900\n",
    "# end = 630\n",
    "end = data.shape[0]\n",
    "\n",
    "with tqdm(total=end - start, initial=start) as pbar:\n",
    "    for val in range(start, end, batch_size):\n",
    "        from_ = val\n",
    "        to_ = min(val + batch_size, end) \n",
    "        pbar.set_description(f\"Processing {from_} - {to_}\")\n",
    "        inputs = \"\".join([f\"\"\"\n",
    "    Comment: {x['text'].strip()}\n",
    "    Index: {i} \n",
    "    Labels: {x[\"labels\"]}\n",
    "    \"\"\" for i, x in data[from_:to_].iterrows()])\n",
    "        try:\n",
    "            resp = client.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.format(inputs=inputs)}],\n",
    "                response_model=List[GenerateExamples],\n",
    "            )\n",
    "            with open(f\"{output_dir}/index-{from_}-{to_}.json\", \"w\", encoding='utf-8') as f:\n",
    "                json.dump([json.loads(res.model_dump_json()) for res in resp], f, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing range {from_} - {to_}:\", e.__str__())\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a930ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{output_dir}/*.json\")\n",
    "\n",
    "def extract_instances_df(instance_data):\n",
    "    labels = instance_data['label']\n",
    "    gen_instances = list(itertools.product(instance_data['generated_comments'], labels or [None]))\n",
    "    instance_df = pd.DataFrame(data=gen_instances, columns=['text', 'target'])\n",
    "    instance_df['value'] = 1 if len(labels) > 0 else 0\n",
    "    return instance_df\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        retrieved = json.load(f)\n",
    "    result_df = pd.concat(list(map(extract_instances_df, retrieved)))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3541e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_df = pd.concat(list(map(process_file, files)))\n",
    "retrieved_df.to_csv(f\"{output_dir}/generated.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
