{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabe1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import itertools\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from lingua import LanguageDetectorBuilder, Language \n",
    "import instructor\n",
    "from vertexai.generative_models import GenerativeModel \n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = \"terraform-training-387507\"\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = \"global\"\n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw/lith_dataset_multi.csv\")\n",
    "ignored_cols = ['text']\n",
    "data['labels'] = data.apply(lambda x: ','.join(k for k in x.keys() if k not in ignored_cols and x[k] == 1), axis=1)\n",
    "selected = data['labels'].apply(lambda x: len(x) > 0) # Select only hate comments\n",
    "data = data[selected]\n",
    "\n",
    "os.makedirs('lthate', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a56d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert in hate speech detection and generation for Lithuanian language.\n",
    "Below there are instances of hate speech comments. Each of them can have one or more labels assigned to it, seperated by comma. This corresponds to multilabel classification task.\n",
    "```\n",
    "{inputs}\n",
    "```\n",
    "For each comment, generate at least 5 comments which have the same semantic meaning and would have the same labels assigned to them. Use rude, foul and explicit language where applicable. \n",
    "Very long comments should be summarized or paraphrased into to shorter ones. The generated comments should vary in tone, severity, or lexical variation. \n",
    "Return them in JSON format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03b23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateExamples(BaseModel):\n",
    "    comment: str\n",
    "    index: int\n",
    "    label: List[str]\n",
    "    generated_comments: List[str]\n",
    "\n",
    "\n",
    "client = instructor.from_vertexai(\n",
    "    client=GenerativeModel(\"gemini-2.5-flash\"),\n",
    "    mode=instructor.Mode.VERTEXAI_TOOLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402ca506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0 - 100:   0%|          | 0/1663 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 1600 - 1663: : 1700it [1:33:55,  3.31s/it]                        \n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "start = 0\n",
    "end = data.shape[0]\n",
    "#end = 1500\n",
    "with tqdm(total=end - start, initial=start) as pbar:\n",
    "    for val in range(start, end, batch_size):\n",
    "        from_ = val\n",
    "        to_ = min(val + batch_size, end) \n",
    "        pbar.set_description(f\"Processing {from_} - {to_}\")\n",
    "        inputs = \"\".join([f\"\"\"\n",
    "    Comment: {x['text']}\n",
    "    Index: {i} \n",
    "    Labels: {x[\"labels\"]}\n",
    "    \"\"\" for i, x in data[from_:to_].iterrows()])\n",
    "        try:\n",
    "            resp = client.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.format(inputs=inputs)}],\n",
    "                response_model=List[GenerateExamples],\n",
    "            )\n",
    "            with open(f\"lthate/index-{from_}-{to_}.json\", \"w\", encoding='utf-8') as f:\n",
    "                json.dump([json.loads(res.model_dump_json()) for res in resp], f, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing range {from_} - {to_}:\", e.__str__())\n",
    "        pbar.update(batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6d53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"lthate/*.json\")\n",
    "\n",
    "def extract_instances_df(instance_data):\n",
    "    labels = instance_data['label']\n",
    "    gen_instances = list(itertools.product(instance_data['generated_comments'], labels or [None]))\n",
    "    instance_df = pd.DataFrame(data=gen_instances, columns=['text', 'target'])\n",
    "    instance_df['value'] = 1 if len(labels) > 0 else 0\n",
    "    return instance_df\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        retrieved = json.load(f)\n",
    "    result_df = pd.concat(list(map(extract_instances_df, retrieved)))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f99dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "retrieved_df = pd.concat(list(map(process_file, files)))\n",
    "retrieved_df['language'] = retrieved_df.apply(lambda x: detector.detect_language_of(x['text']), axis=1)\n",
    "retrieved_df = retrieved_df[retrieved_df['language'] == Language.LITHUANIAN].drop(labels='language', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c88435df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decensore(text):\n",
    "    replacements = {\n",
    "        'n***ui': 'nachui',\n",
    "        'n**ui': 'nahui',\n",
    "        'b**t': 'blet',\n",
    "        'p**ti': 'pisti',\n",
    "        'py*****': 'pyderai',\n",
    "        'n*ger': 'niger',\n",
    "        'š*d': 'šūd'\n",
    "    }\n",
    "    for replaced, replacement in replacements.items():\n",
    "        text = re.sub(replaced.replace('*', r'\\*'), replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "retrieved_df['text'] = retrieved_df['text'].apply(decensore)\n",
    "retrieved_df.to_csv(\"lthate/generated.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
